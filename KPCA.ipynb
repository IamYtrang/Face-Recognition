{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing libraries**"
      ],
      "metadata": {
        "id": "9cDM1n0ZLdWg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8tZ23SBJwgA",
        "outputId": "4c8c68c6-1b12-4acc-dca9-ee065447e960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import random\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n"
      ],
      "metadata": {
        "id": "swQ6dKELLZEf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gulpDmI-LcB0",
        "outputId": "e48c341c-ea11-43fd-dad1-fd2bf8f4d691"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD DATASET"
      ],
      "metadata": {
        "id": "aAkvhCDpNlro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir):\n",
        "    faces = defaultdict(list)\n",
        "    for emotion in os.listdir(data_dir):\n",
        "        emotion_dir = os.path.join(data_dir, emotion)\n",
        "        if os.path.isdir(emotion_dir):\n",
        "            for image_file in os.listdir(emotion_dir):\n",
        "                image_path = os.path.join(emotion_dir, image_file)\n",
        "                image = Image.open(image_path).convert(\"L\")\n",
        "                image = np.array(image).flatten()\n",
        "                faces[emotion].append(image)\n",
        "    return faces"
      ],
      "metadata": {
        "id": "i0B_m1ukMb_r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faces = load_data(\"/content/drive/MyDrive/Project  Face recognition/Demo/ATM images\")"
      ],
      "metadata": {
        "id": "MZ0Pxt7bNo_r"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_label_images(faces, label):\n",
        "    images = faces[label]\n",
        "    num_images = len(images)\n",
        "    num_cols = math.ceil(math.sqrt(num_images))\n",
        "    num_rows = math.ceil(num_images / num_cols)\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n",
        "    axes = axes.flatten() if num_images > 1 else [axes]\n",
        "\n",
        "    for ax, img in zip(axes, images):\n",
        "        ax.imshow(img.reshape((112, 92)), cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    for ax in axes[num_images:]:\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "EwNZkswRN_yy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_person_name(image_path):\n",
        "    parts = image_path.replace(\"\\\\\", \"/\").split(\"/\")\n",
        "    person_label = parts[-2]\n",
        "    img_num = int(parts[-1].split('.')[0])\n",
        "    return person_label, img_num"
      ],
      "metadata": {
        "id": "Wgr1U7C0O4qe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test(faces, train_ratio):\n",
        "    training_set = defaultdict(list)\n",
        "    testing_set = defaultdict(list)\n",
        "\n",
        "    for label, images in faces.items():\n",
        "        random.shuffle(images)\n",
        "        num_train = int(len(images) * train_ratio)\n",
        "\n",
        "        for img_index, image in enumerate(images):\n",
        "            person_label, img_num = get_person_name(label + '/' + str(img_index+1) + '.jpg')\n",
        "            if img_index < num_train:\n",
        "                training_set[person_label].append(image)\n",
        "            else:\n",
        "                testing_set[person_label].append(image)\n",
        "\n",
        "    return training_set, testing_set"
      ],
      "metadata": {
        "id": "bwGcW-TPO8Nh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set, testing_set = split_train_test(faces, 0.9)"
      ],
      "metadata": {
        "id": "pWQkHE5VP0PC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(dataset):\n",
        "    X = []\n",
        "    y = []\n",
        "    for person_label, images in dataset.items():\n",
        "        for image in images:\n",
        "            X.append(image)\n",
        "            y.append(person_label)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "PKqlw0AwPGH8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = prepare_data(training_set)\n",
        "X_test, y_test = prepare_data(training_set)"
      ],
      "metadata": {
        "id": "hkeotDASPzK1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    ('kpca', KernelPCA(fit_inverse_transform=True)),\n",
        "    ('svc', SVC())\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'kpca__kernel': ['rbf', 'sigmoid', 'poly', 'linear'],\n",
        "    'kpca__gamma': np.logspace(-2, 2, 5),\n",
        "    'kpca__n_components': [10, 50, 60, 100, 150],\n",
        "    'svc__C': [0.1, 1, 10, 100, 1000],\n",
        "    'svc__gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROgoXzn7RZvd",
        "outputId": "c79ffc56-dd76-47b6-99e5-605965b8adcd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "250 fits failed out of a total of 5000.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "250 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 401, in fit\n",
            "    Xt = self._fit(X, y, **fit_params_steps)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 353, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_kernel_pca.py\", line 456, in fit_transform\n",
            "    self.fit(X, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_kernel_pca.py\", line 430, in fit\n",
            "    self._fit_inverse_transform(X_transformed, X)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_kernel_pca.py\", line 397, in _fit_inverse_transform\n",
            "    self.dual_coef_ = linalg.solve(K, X, assume_a=\"pos\", overwrite_a=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_basic.py\", line 254, in solve\n",
            "    _solve_check(n, info)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scipy/linalg/_basic.py\", line 41, in _solve_check\n",
            "    raise LinAlgError('Matrix is singular.')\n",
            "numpy.linalg.LinAlgError: Matrix is singular.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.01388889 0.01388889 0.01388889 0.01388889 0.025      0.01388889\n",
            " 0.025      0.01388889 0.025      0.01388889 0.01388889 0.01388889\n",
            " 0.01944444 0.01388889 0.025      0.01388889 0.025      0.01388889\n",
            " 0.025      0.02222222 0.01388889 0.01388889 0.01666667 0.01388889\n",
            " 0.025      0.01388889 0.025      0.01388889 0.025      0.025\n",
            " 0.01388889 0.01388889 0.01944444 0.01388889 0.025      0.01388889\n",
            " 0.025      0.01388889 0.025      0.01944444 0.01388889 0.01388889\n",
            " 0.02222222 0.01388889 0.025      0.01388889 0.025      0.01388889\n",
            " 0.025      0.02222222 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan 0.11111111 0.01388889 0.92777778 0.01388889\n",
            " 0.97222222 0.01388889 0.97222222 0.01388889 0.97222222 0.01388889\n",
            " 0.11111111 0.01388889 0.93333333 0.01388889 0.97222222 0.01388889\n",
            " 0.97222222 0.01388889 0.97222222 0.01388889 0.10833333 0.01388889\n",
            " 0.94444444 0.01388889 0.96944444 0.01388889 0.96944444 0.01388889\n",
            " 0.96944444 0.01388889 0.11111111 0.01388889 0.94444444 0.01388889\n",
            " 0.96388889 0.01388889 0.96388889 0.01388889 0.96388889 0.01388889\n",
            " 0.10833333 0.01388889 0.90555556 0.01388889 0.95555556 0.01388889\n",
            " 0.95277778 0.01388889 0.95277778 0.01388889 0.11111111 0.01388889\n",
            " 0.96388889 0.01388889 0.975      0.01388889 0.975      0.01388889\n",
            " 0.975      0.01388889 0.11111111 0.01388889 0.96666667 0.01388889\n",
            " 0.975      0.01388889 0.975      0.01388889 0.975      0.01388889\n",
            " 0.11111111 0.01388889 0.975      0.01388889 0.97777778 0.01388889\n",
            " 0.97777778 0.01388889 0.97777778 0.01388889 0.11111111 0.01388889\n",
            " 0.96666667 0.01388889 0.97777778 0.01388889 0.97777778 0.01388889\n",
            " 0.97777778 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.025      0.01388889 0.025      0.01388889 0.025      0.01388889\n",
            " 0.01388889 0.01388889 0.01944444 0.01388889 0.025      0.01388889\n",
            " 0.025      0.01388889 0.025      0.02222222 0.01388889 0.01388889\n",
            " 0.01666667 0.01388889 0.025      0.01388889 0.025      0.01388889\n",
            " 0.025      0.025      0.01388889 0.01388889 0.01944444 0.01388889\n",
            " 0.025      0.01388889 0.025      0.01388889 0.025      0.01944444\n",
            " 0.01388889 0.01388889 0.02222222 0.01388889 0.025      0.01388889\n",
            " 0.025      0.01388889 0.025      0.02222222 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan 0.11111111 0.01388889\n",
            " 0.92777778 0.01388889 0.97222222 0.01388889 0.97222222 0.01388889\n",
            " 0.97222222 0.01388889 0.11111111 0.01388889 0.93333333 0.01388889\n",
            " 0.97222222 0.01388889 0.97222222 0.01388889 0.97222222 0.01388889\n",
            " 0.10833333 0.01388889 0.94444444 0.01388889 0.96944444 0.01388889\n",
            " 0.96944444 0.01388889 0.96944444 0.01388889 0.11111111 0.01388889\n",
            " 0.94444444 0.01388889 0.96388889 0.01388889 0.96388889 0.01388889\n",
            " 0.96388889 0.01388889 0.10833333 0.01388889 0.90555556 0.01388889\n",
            " 0.95555556 0.01388889 0.95277778 0.01388889 0.95277778 0.01388889\n",
            " 0.11111111 0.01388889 0.96388889 0.01388889 0.975      0.01388889\n",
            " 0.975      0.01388889 0.975      0.01388889 0.11111111 0.01388889\n",
            " 0.96666667 0.01388889 0.975      0.01388889 0.975      0.01388889\n",
            " 0.975      0.01388889 0.11111111 0.01388889 0.975      0.01388889\n",
            " 0.97777778 0.01388889 0.97777778 0.01388889 0.97777778 0.01388889\n",
            " 0.11111111 0.01388889 0.96666667 0.01388889 0.97777778 0.01388889\n",
            " 0.97777778 0.01388889 0.97777778 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.025      0.01388889 0.025      0.01388889\n",
            " 0.025      0.01388889 0.01388889 0.01388889 0.01944444 0.01388889\n",
            " 0.025      0.01388889 0.025      0.01388889 0.025      0.02222222\n",
            " 0.01388889 0.01388889 0.01666667 0.01388889 0.025      0.01388889\n",
            " 0.025      0.01388889 0.025      0.025      0.01388889 0.01388889\n",
            " 0.01944444 0.01388889 0.025      0.01388889 0.025      0.01388889\n",
            " 0.025      0.01944444 0.01388889 0.01388889 0.02222222 0.01388889\n",
            " 0.025      0.01388889 0.025      0.01388889 0.025      0.02222222\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.11111111 0.01388889 0.92777778 0.01388889 0.97222222 0.01388889\n",
            " 0.97222222 0.01388889 0.97222222 0.01388889 0.11111111 0.01388889\n",
            " 0.93333333 0.01388889 0.97222222 0.01388889 0.97222222 0.01388889\n",
            " 0.97222222 0.01388889 0.10833333 0.01388889 0.94444444 0.01388889\n",
            " 0.96944444 0.01388889 0.96944444 0.01388889 0.96944444 0.01388889\n",
            " 0.11111111 0.01388889 0.94444444 0.01388889 0.96388889 0.01388889\n",
            " 0.96388889 0.01388889 0.96388889 0.01388889 0.10833333 0.01388889\n",
            " 0.90555556 0.01388889 0.95555556 0.01388889 0.95277778 0.01388889\n",
            " 0.95277778 0.01388889 0.11111111 0.01388889 0.96388889 0.01388889\n",
            " 0.975      0.01388889 0.975      0.01388889 0.975      0.01388889\n",
            " 0.11111111 0.01388889 0.96666667 0.01388889 0.975      0.01388889\n",
            " 0.975      0.01388889 0.975      0.01388889 0.11111111 0.01388889\n",
            " 0.975      0.01388889 0.97777778 0.01388889 0.97777778 0.01388889\n",
            " 0.97777778 0.01388889 0.11111111 0.01388889 0.96666667 0.01388889\n",
            " 0.97777778 0.01388889 0.97777778 0.01388889 0.97777778 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.025      0.01388889\n",
            " 0.025      0.01388889 0.025      0.01388889 0.01388889 0.01388889\n",
            " 0.01944444 0.01388889 0.025      0.01388889 0.025      0.01388889\n",
            " 0.025      0.02222222 0.01388889 0.01388889 0.01666667 0.01388889\n",
            " 0.025      0.01388889 0.025      0.01388889 0.025      0.025\n",
            " 0.01388889 0.01388889 0.01944444 0.01388889 0.025      0.01388889\n",
            " 0.025      0.01388889 0.025      0.01944444 0.01388889 0.01388889\n",
            " 0.02222222 0.01388889 0.025      0.01388889 0.025      0.01388889\n",
            " 0.025      0.02222222 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan 0.11111111 0.01388889 0.92777778 0.01388889\n",
            " 0.97222222 0.01388889 0.97222222 0.01388889 0.97222222 0.01388889\n",
            " 0.11111111 0.01388889 0.93333333 0.01388889 0.97222222 0.01388889\n",
            " 0.97222222 0.01388889 0.97222222 0.01388889 0.10833333 0.01388889\n",
            " 0.94444444 0.01388889 0.96944444 0.01388889 0.96944444 0.01388889\n",
            " 0.96944444 0.01388889 0.11111111 0.01388889 0.94444444 0.01388889\n",
            " 0.96388889 0.01388889 0.96388889 0.01388889 0.96388889 0.01388889\n",
            " 0.10833333 0.01388889 0.90555556 0.01388889 0.95555556 0.01388889\n",
            " 0.95277778 0.01388889 0.95277778 0.01388889 0.11111111 0.01388889\n",
            " 0.96388889 0.01388889 0.975      0.01388889 0.975      0.01388889\n",
            " 0.975      0.01388889 0.11111111 0.01388889 0.96666667 0.01388889\n",
            " 0.975      0.01388889 0.975      0.01388889 0.975      0.01388889\n",
            " 0.11111111 0.01388889 0.975      0.01388889 0.97777778 0.01388889\n",
            " 0.97777778 0.01388889 0.97777778 0.01388889 0.11111111 0.01388889\n",
            " 0.96666667 0.01388889 0.97777778 0.01388889 0.97777778 0.01388889\n",
            " 0.97777778 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.025      0.01388889 0.025      0.01388889 0.025      0.01388889\n",
            " 0.01388889 0.01388889 0.01944444 0.01388889 0.025      0.01388889\n",
            " 0.025      0.01388889 0.025      0.02222222 0.01388889 0.01388889\n",
            " 0.01666667 0.01388889 0.025      0.01388889 0.025      0.01388889\n",
            " 0.025      0.025      0.01388889 0.01388889 0.01944444 0.01388889\n",
            " 0.025      0.01388889 0.025      0.01388889 0.025      0.01944444\n",
            " 0.01388889 0.01388889 0.02222222 0.01388889 0.025      0.01388889\n",
            " 0.025      0.01388889 0.025      0.02222222 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            " 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889 0.01388889\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan 0.11111111 0.01388889\n",
            " 0.92777778 0.01388889 0.97222222 0.01388889 0.97222222 0.01388889\n",
            " 0.97222222 0.01388889 0.11111111 0.01388889 0.93333333 0.01388889\n",
            " 0.97222222 0.01388889 0.97222222 0.01388889 0.97222222 0.01388889\n",
            " 0.10833333 0.01388889 0.94444444 0.01388889 0.96944444 0.01388889\n",
            " 0.96944444 0.01388889 0.96944444 0.01388889 0.11111111 0.01388889\n",
            " 0.94444444 0.01388889 0.96388889 0.01388889 0.96388889 0.01388889\n",
            " 0.96388889 0.01388889 0.10833333 0.01388889 0.90555556 0.01388889\n",
            " 0.95555556 0.01388889 0.95277778 0.01388889 0.95277778 0.01388889\n",
            " 0.11111111 0.01388889 0.96388889 0.01388889 0.975      0.01388889\n",
            " 0.975      0.01388889 0.975      0.01388889 0.11111111 0.01388889\n",
            " 0.96666667 0.01388889 0.975      0.01388889 0.975      0.01388889\n",
            " 0.975      0.01388889 0.11111111 0.01388889 0.975      0.01388889\n",
            " 0.97777778 0.01388889 0.97777778 0.01388889 0.97777778 0.01388889\n",
            " 0.11111111 0.01388889 0.96666667 0.01388889 0.97777778 0.01388889\n",
            " 0.97777778 0.01388889 0.97777778 0.01388889]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'kpca__gamma': 0.01, 'kpca__kernel': 'linear', 'kpca__n_components': 100, 'svc__C': 10, 'svc__gamma': 'scale'}\n",
            "Best cross-validation score: 0.9777777777777779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "DMu89lFxRfDb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY3DaGQNRutu",
        "outputId": "89a2ea32-0a45-4c5d-c20c-71b73898e5f3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          s1       1.00      1.00      1.00         9\n",
            "         s10       1.00      1.00      1.00         9\n",
            "         s11       1.00      1.00      1.00         9\n",
            "         s12       1.00      1.00      1.00         9\n",
            "         s13       1.00      1.00      1.00         9\n",
            "         s14       1.00      1.00      1.00         9\n",
            "         s15       1.00      1.00      1.00         9\n",
            "         s16       1.00      1.00      1.00         9\n",
            "         s17       1.00      1.00      1.00         9\n",
            "         s18       1.00      1.00      1.00         9\n",
            "         s19       1.00      1.00      1.00         9\n",
            "          s2       1.00      1.00      1.00         9\n",
            "         s20       1.00      1.00      1.00         9\n",
            "         s21       1.00      1.00      1.00         9\n",
            "         s22       1.00      1.00      1.00         9\n",
            "         s23       1.00      1.00      1.00         9\n",
            "         s24       1.00      1.00      1.00         9\n",
            "         s25       1.00      1.00      1.00         9\n",
            "         s26       1.00      1.00      1.00         9\n",
            "         s27       1.00      1.00      1.00         9\n",
            "         s28       1.00      1.00      1.00         9\n",
            "         s29       1.00      1.00      1.00         9\n",
            "          s3       1.00      1.00      1.00         9\n",
            "         s30       1.00      1.00      1.00         9\n",
            "         s31       1.00      1.00      1.00         9\n",
            "         s32       1.00      1.00      1.00         9\n",
            "         s33       1.00      1.00      1.00         9\n",
            "         s34       1.00      1.00      1.00         9\n",
            "         s35       1.00      1.00      1.00         9\n",
            "         s36       1.00      1.00      1.00         9\n",
            "         s37       1.00      1.00      1.00         9\n",
            "         s38       1.00      1.00      1.00         9\n",
            "         s39       1.00      1.00      1.00         9\n",
            "          s4       1.00      1.00      1.00         9\n",
            "         s40       1.00      1.00      1.00         9\n",
            "          s5       1.00      1.00      1.00         9\n",
            "          s6       1.00      1.00      1.00         9\n",
            "          s7       1.00      1.00      1.00         9\n",
            "          s8       1.00      1.00      1.00         9\n",
            "          s9       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           1.00       360\n",
            "   macro avg       1.00      1.00      1.00       360\n",
            "weighted avg       1.00      1.00      1.00       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "\n",
        "misclassified_images = X_test[misclassified_indices]\n",
        "misclassified_true_labels = y_test[misclassified_indices]\n",
        "misclassified_pred_labels = y_pred[misclassified_indices]\n",
        "\n",
        "num_misclassified = len(misclassified_indices)\n",
        "num_misclassified"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY8P6fmCSv2J",
        "outputId": "9a402a0f-f0ad-469b-f0c2-b887f1ee73cf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_images_to_show = 10\n",
        "\n",
        "for i, (image, true_label, pred_label) in enumerate(zip(misclassified_images, misclassified_true_labels, misclassified_pred_labels)):\n",
        "    if i >= num_images_to_show:\n",
        "        break\n",
        "\n",
        "    plt.subplot(2, num_images_to_show // 2, i + 1)\n",
        "    plt.imshow(image.reshape(112, 92), cmap='gray')\n",
        "    plt.title(f'True: {true_label}\\nPredicted: {pred_label}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "m1z1qx_hS9s_",
        "outputId": "7e4ee53e-924b-4b3a-e993-2b179e2add86"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QfZy7TCmcoEW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}